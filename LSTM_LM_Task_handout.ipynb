{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3R04Si0M8KzK"
   },
   "source": [
    "# LSTM nyelvmodell\n",
    "\n",
    "Az inspiráció nem más, mint  [Andrej Karpathy híres írása](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
    "\n",
    "\n",
    "## Adatbeolvasás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T16:48:43.620252Z",
     "start_time": "2018-10-06T16:48:43.607851Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "egfH6gkf8WQg",
    "outputId": "67e0a903-02eb-480f-c9b3-f06f90336115"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1212)\n",
    "\n",
    "tf.random.set_random_seed(1234)\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# This can be an important parameter, so be aware of it...\n",
    "max_seq_length = 15\n",
    "max_num_of_sents = 57200\n",
    "# max_num_of_sents = 50 # How many sentences should we read from the corpus (max=57200)\n",
    "\n",
    "def generate_brown_word_to_id_map():\n",
    "    \"\"\"Return a dictionary mapping downcased Brown-words to their ids.\n",
    "    Numbering starts from 1 since we use 0 for masking (!!!).\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    for word in brown.words():\n",
    "        words.add(word.lower())\n",
    "    return {word: idx + 1 for idx, word in enumerate(sorted(words))}\n",
    "\n",
    "\n",
    "class BrownReader:\n",
    "    \"\"\"A reader class for the Brown corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word_to_id_map = generate_brown_word_to_id_map()\n",
    "        self.id_to_word_map = {idx: word for word, idx in self.word_to_id_map.items()}\n",
    "\n",
    "    def n_words(self):\n",
    "        return len(self.word_to_id_map)\n",
    "\n",
    "    def sentence_to_ids(self, sentence):\n",
    "        \"\"\"Return the word ids of a sentence.\n",
    "        \"\"\"\n",
    "        return [self.word_to_id_map[word.lower()] for word in sentence]\n",
    "        \n",
    "    def sentences(self):\n",
    "        \"\"\"Generator yielding features from the Brown corpus.\n",
    "        \"\"\"\n",
    "        return (self.sentence_to_ids(sentence) for sentence in brown.sents())\n",
    "\n",
    "    def sentence_matrixes(self):\n",
    "        x = np.zeros((max_num_of_sents, max_seq_length-1))\n",
    "        y = np.zeros((max_num_of_sents, max_seq_length-1))\n",
    "        sents = self.sentences()\n",
    "        for idx, sent in enumerate(sents):\n",
    "            if idx == max_num_of_sents:\n",
    "                break\n",
    "            np_array = np.asarray(sent)\n",
    "            length  = min(max_seq_length, len(np_array))\n",
    "            x[idx, :length - 1] = np_array[:length - 1]\n",
    "            y[idx, :length - 1] = np_array[1:length]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pvjdF2_p9_YV"
   },
   "source": [
    "## Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SavNwc5m9_Ya"
   },
   "source": [
    "### Paraméterek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXHNzCXj9_Ye"
   },
   "outputs": [],
   "source": [
    "br = BrownReader()\n",
    "n_words = br.n_words()\n",
    "\n",
    "max_input_length = max_seq_length - 1 # since our x/y input does not contain the last/first element of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JkhDDIwl9_ZC"
   },
   "outputs": [],
   "source": [
    "data_x, data_y = br.sentence_matrixes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJdzjP6R9_ZK"
   },
   "outputs": [],
   "source": [
    "data_y = np.expand_dims(data_y, -1) # It seems that Keras needs this for the \"one-cold\" and softmax dims to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feladatok\n",
    "\n",
    "Alább"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# háló paraméterek\n",
    "\n",
    "lstm_size = ???\n",
    "embedding_size = ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWPOCc6q9_Yn"
   },
   "source": [
    "### Háló"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "FXth0CDm9_Yq",
    "outputId": "915247f0-9294-4cb8-c3d7-ed48d4cc0cf8"
   },
   "outputs": [],
   "source": [
    "# Import:\n",
    "# Importáld a megfelelő rétegeket\n",
    "# Gondolj bele, hogy a hálóban az első réteg egy \"beágazás\" kell majd legyen\n",
    "# Ne feledd behozni a funkcionális vagy szekvenciális API-nak megfelelő \"fő\" osztályt\n",
    "# Adott esetben az optimalizálót\n",
    "# Nem külömben a \"bakcendet\", hogy jó gyakorlat szerint reseteld a gráfot\n",
    "# valmint a ritka, kategorikus keresztentrópiát, mint veszteségfüggvényt\n",
    "\n",
    "# jó gyakorlat: reseteld a gráfot!\n",
    "\n",
    "# Model\n",
    "########\n",
    "# Építs modellt!\n",
    "# Bemeneti réteggel kezdd, aminek az inputja egy maximális szöveghosszt jelző vektor. \n",
    "# Meg ugye hogy akár vektor, akár nem, a shape az tuple...\n",
    "# Ezt kövesse egy beágyazás réteg.\n",
    "# FIGYELEM: \n",
    "# 1. a szélessége a szavak száma plusz 1\n",
    "# 2. mérete paraméterben adott, lásd fent\n",
    "# 3. bemenet hossza: legnagyobb bemenet mérete -1\n",
    "# 4. a nulla értékek maszkolandóak benne\n",
    "# FELADAT: Ezeket indokold meg, miért?\n",
    "\n",
    "# Következő KÉT rétegben LSTM-ek legyenek.\n",
    "# Ugye ahhoz, hogy egymásra építhetőek legyenek, nem csak a szekvencia végi predikcióikat kell visszaadják\n",
    "# Erre van valahol egy csinos paraméter... ;-)\n",
    "\n",
    "# Végül pedig egy fully connected layerrel és softmaxxal projektáljuk a kimenetet.\n",
    "# Mennyi a szélessége? (Segítség: ha a fentieket jól megindokoltad, akkor már tudod. ;-)\n",
    "\n",
    "# Végül példányosítsuk a modellt!\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKXuebno9_Y1"
   },
   "source": [
    "### Hiba, optimalizáció és modelfordítás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Mob9-6s9_Y3"
   },
   "outputs": [],
   "source": [
    "# Loss \n",
    "\n",
    "loss = ??? # One-hot enkódolt kimenetünk van. Mit is használunk?\n",
    "\n",
    "# Optimizer\n",
    "optimizer = ??? #Ízlés szerint...\n",
    " \n",
    "# Compilation\n",
    "#############\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLLqU3c_9_Y8"
   },
   "source": [
    "### Tréning\n",
    "\n",
    "Előállítjuk a tréningadatot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMPFHd-M9_ZX"
   },
   "source": [
    "És trénelünk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "EJboxaY_9_Zd",
    "outputId": "80dacba0-9177-4b12-cbd6-641659f376f0"
   },
   "outputs": [],
   "source": [
    "# Illesszük az adatra a modellt. Használjunk 10% validációt\n",
    "# - nyelvmodellnél ez nem olyan lényeges\n",
    "# Használhatjuk a Keras beépített validációs splitjét.\n",
    "# Adjunk meg reális batch méretet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eao03t2U9gBy"
   },
   "source": [
    "## Demó 1: Következő szó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T17:04:58.584741Z",
     "start_time": "2018-10-06T16:48:43.630Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "RM1WVrua9qaY",
    "outputId": "275659e7-1141-414a-c73b-cd25d8c599c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "############\n",
    "\n",
    "def str_to_input(s):\n",
    "    \"\"\"Convert a string to appropriate model input.\n",
    "    \"\"\"\n",
    "    words = [x.lower() for x in s.split()[:max_input_length]]\n",
    "    ids = [br.word_to_id_map[word] for word in words]\n",
    "    ids_array = np.asarray(ids)\n",
    "    length = min(max_input_length, len(ids_array))\n",
    "    result = np.zeros((1, max_input_length))\n",
    "    result[0, :length] = ids_array[:length]\n",
    "    return result, length\n",
    "    \n",
    "\n",
    "while True:\n",
    "    s = input(\"\\nEnter a few starting words of a sentence or <return> to stop: \")\n",
    "    if s == \"\":\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            x, length = str_to_input(s)\n",
    "            predictions = model.predict(x)\n",
    "            probs = predictions[0][length - 1]\n",
    "            most_probable = np.argmax(probs)\n",
    "            print(\"Predicted next word:\", br.id_to_word_map[most_probable])\n",
    "        except KeyError:\n",
    "            print(\"Unknown words -- please try again!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4AkEJxBzPfm"
   },
   "source": [
    "## Demó 2: Mondatok hasonlósága\n",
    "\n",
    "Először egy függvényt definiálunk, amely előállítja a rejtett LSTM állapotokat egy inputból:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thYtAla6gDbi"
   },
   "outputs": [],
   "source": [
    "input_layer = model.get_layer(\"input_1\")\n",
    "lstm_2_layer = model.get_layer(\"lstm_1\")\n",
    "\n",
    "hidden_states_fun = K.function([input_layer.input],[lstm_2_layer.output])\n",
    "\n",
    "def get_embedding(x, timestep):\n",
    "    \"\"\"Return the hidden state associated with an input at the given timestep.\n",
    "    \"\"\"\n",
    "    hidden_states = hidden_states_fun([x])[0]\n",
    "    return hidden_states[0, timestep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "LKUgmCrG7Itw",
    "outputId": "ae970159-4ca0-48c8-ba74-cb967c085aed"
   },
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "while True:\n",
    "    s1 = input(\"\\nEnter the first sentence or <return> to quit: \")\n",
    "    if s1 == \"\": break\n",
    "    s2 = input(\"\\nEnter the second sentence: \")\n",
    "    try:\n",
    "        x1, l1 = str_to_input(s1)\n",
    "        x2, l2 = str_to_input(s2)\n",
    "        e1 = get_embedding(x1, l1-1)\n",
    "        e2 = get_embedding(x2, l2-1)\n",
    "        print(\"The cosine similarity between the two sentences is\", cos_sim(e1, e2))\n",
    "    except KeyError:\n",
    "        print(\"Unknown words -- please try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-3W1oxZv4DI"
   },
   "source": [
    "## Demó 3: Mini search engine\n",
    "\n",
    "A Spotify által kiadott [Annoy](https://github.com/spotify/annoy) library segítségével felhasználjuk a tréningezett modellt arra, hogy a Brown corpus minden mondatához egy LSTM belső reprezentációjából származó vektort rendeljünk, majd szomszédossági kereséssel mini keresőt hozzunk létre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWp-rofueeS0"
   },
   "outputs": [],
   "source": [
    "def brown_sent_to_input(ids):\n",
    "  ids_array = np.asarray(ids)\n",
    "  length = min(max_input_length, len(ids_array))\n",
    "  result = np.zeros((1, max_input_length))\n",
    "  result[0, :length] = ids_array[:length]\n",
    "  return result, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wda7o8ntfov1"
   },
   "outputs": [],
   "source": [
    "sentlist = list(br.sentences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "eaAJ7Pz5i8Cp",
    "outputId": "f94ff59a-e327-4d02-e725-66ea3e5682fc"
   },
   "outputs": [],
   "source": [
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8SO85y_w8sX"
   },
   "outputs": [],
   "source": [
    "INDEX_COVERAGE_PERCENT = 1.0 #How much of the corpus you want ot index? 1.0 means whole, 0.5 means half.\n",
    "NEAREST_NEIGHBOR_NUM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UfwnfqYff6h8",
    "outputId": "930b77b2-3ce1-4b5d-bbd4-03e57b40fdb7"
   },
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm\n",
    "\n",
    "index = AnnoyIndex(512, metric=\"angular\")\n",
    "\n",
    "for i in tqdm(range(len(int(sentlist*INDEX_COVERAGE_PERCENT)))):\n",
    "  inputs,length = brown_sent_to_input(sentlist[i])\n",
    "  vector = get_embedding(inputs, length-1)\n",
    "  index.add_item(i,vector)\n",
    "\n",
    "print(\"Building index...\")\n",
    "index.build(100)\n",
    "print(\"Index done, ready to query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ex6AGjVrh6Sb"
   },
   "outputs": [],
   "source": [
    "def print_brown_index(sentences, indices):\n",
    "  for i in indices:\n",
    "    word_ids_list = sentences[i]\n",
    "    for j in word_ids_list:\n",
    "      print(br.id_to_word_map[j]+\" \", end='')\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "colab_type": "code",
    "id": "V6y-1aVGi573",
    "outputId": "68d4ef41-2398-4e56-a170-25bae99df5c7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "  query = input(\"\\nEnter the query or <return> to quit: \")\n",
    "  if query == \"\": break\n",
    "  try:\n",
    "    in_ids, length = str_to_input(query)\n",
    "    in_vector = get_embedding(in_ids, length-1)\n",
    "    nearest_sentence_indices = index.get_nns_by_vector(in_vector, NEAREST_NEIGHBOR_NUM)\n",
    "    #print(\"nearest indices:\", nearest_sentence_indices)\n",
    "    print_brown_index(sentlist, nearest_sentence_indices)\n",
    "\n",
    "  except KeyError:\n",
    "    print(\"Unknown words -- please try again!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_LM_Task.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
