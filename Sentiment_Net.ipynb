{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvGdRkEvXT4lzFVV69yC0c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/blob/main/Sentiment_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS8T-dxnwfjO"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDQLvAy5gI0C"
      },
      "source": [
        "##Szószedet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDUZMjUkgIIe",
        "outputId": "7d2403c9-286a-4bdf-cf6b-d44717195878"
      },
      "source": [
        "!rm *zip*\n",
        "!wget https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
        "!unzip twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
        "df=pd.read_csv(\"TEST.csv\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-06 17:36:29--  https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip [following]\n",
            "--2021-10-06 17:36:29--  https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1258171 (1.2M) [application/zip]\n",
            "Saving to: ‘twitter_sentiment_analysis_ai_challenge-dataset.zip’\n",
            "\n",
            "twitter_sentiment_a 100%[===================>]   1.20M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-10-06 17:36:30 (113 MB/s) - ‘twitter_sentiment_analysis_ai_challenge-dataset.zip’ saved [1258171/1258171]\n",
            "\n",
            "Archive:  twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
            "replace SampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: SampleSubmission.csv    \n",
            "  inflating: TEST.csv                \n",
            "  inflating: TRAIN.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDtEcJB5haQF",
        "outputId": "85fe23d3-94e3-47b0-9c64-28ad277f2e84"
      },
      "source": [
        "df.head()\n",
        "alltext1=df.text\n",
        "print (len(alltext1))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA8Mk878pUdU",
        "outputId": "6cf63d85-5116-45dc-d498-0d0f959007d6"
      },
      "source": [
        "df2=pd.read_csv(\"TRAIN.csv\")\n",
        "alltext2=df2.text\n",
        "print (len(alltext2))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P-xoaRAp9gy",
        "outputId": "81c12814-4ba1-4ca9-b8e4-cbddde206696"
      },
      "source": [
        "alltext1=list(alltext1)\n",
        "alltext2=list(alltext2)\n",
        "alltext=alltext1+alltext2\n",
        "print(f\" {len(alltext)}\")\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 14640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OczaGqbTqKgz"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vGSDdwGqneG"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuRrm2HJja_6",
        "outputId": "2d3e1a7a-56cf-45b5-ff9e-a8116a90fb54"
      },
      "source": [
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "print(spell.correction(\"@frustrating\"))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "frustrating\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCN90l7Ijkb5"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_URL(text):\n",
        "    \"\"\"Remove URLs from a text string\"\"\"\n",
        "    return re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "def is_number(inp:str):\n",
        "    a=re.search(\"[\\d]\",inp)\n",
        "    if a==None: \n",
        "        return(False)\n",
        "    return(True)\n",
        "\n",
        "def strip_all_entities(text):\n",
        "    import re,string\n",
        "    entity_prefixes = ['“','@','#']\n",
        "    for separator in  string.punctuation:\n",
        "        if separator not in entity_prefixes :\n",
        "            text = text.replace(separator,' ')\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        word = word.strip()\n",
        "        if word:\n",
        "            if word[0] not in entity_prefixes:\n",
        "                words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def query_sentence(index):\n",
        "    print(f\"orig: {alltext[index]}\")\n",
        "    spell=SpellChecker()\n",
        "    inp=alltext[index]\n",
        "    inp=remove_URL(inp)\n",
        "    inp=remove_emoji(inp)\n",
        "    inp=strip_all_entities(inp)\n",
        "    \n",
        "    txt=inp.replace(\",\",\" \").replace(\".\",\" \").replace(\"!\",\" \").replace(\"#\",\" \").replace(\"???\",\" \").replace(\"??\",\" \").replace(\"?\",\" \").replace(\"  \",\" \").replace(\"  \",\" \").replace('\"',\"\").replace(\"“\",\"\")\n",
        "\n",
        "    #print(txt)\n",
        "    normal=\"\"\n",
        "    for i in txt.split(\" \"):\n",
        "        w1=i.lower().strip()\n",
        "        if w1 !=\"\":\n",
        "            w2=spell.correction(w1).strip()\n",
        "        else:\n",
        "            w2=\"\"\n",
        "        if w2==\"i\":\n",
        "            pass\n",
        "            #print(f\"chek: {w1}, {w2}\")\n",
        "        if is_number(w2):\n",
        "            w2=\"\"    \n",
        "        normal += w2+\" \"\n",
        "\n",
        "    print(f\"  conv: {normal}\")\n",
        "    return(normal)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzMOTKbN2G3R"
      },
      "source": [
        "##Mentés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9ZUtwPWj2wv"
      },
      "source": [
        "t=[]\n",
        "for i in range(len(alltext)):\n",
        "    print(f\"{i}\", end=\" \")\n",
        "    t.append(query_sentence(i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivPXsq3JoP0m",
        "outputId": "8efae1f7-0490-46c3-fbfb-488acad3111d"
      },
      "source": [
        "s=set()\n",
        "for sent in t:\n",
        "    wlist=sent.strip().split()\n",
        "    for w in wlist:\n",
        "        s.add(w)\n",
        "print (len(s))\n",
        "ordered=list(s)\n",
        "ordered.sort()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi9MDCpq6bJv"
      },
      "source": [
        "print(ordered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIoSRwse2GOY"
      },
      "source": [
        "sentence=pd.DataFrame(t)\n",
        "sentence.columns=[\"Sentence\"]\n",
        "sentence.to_csv(\"Sentences.csv\")"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbDPRdUc6gnp"
      },
      "source": [
        "wordlist=pd.DataFrame(ordered)\n",
        "wordlist.columns=[\"Words\"]\n",
        "wordlist.to_csv(\"words.csv\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RCD1OKy7kJr"
      },
      "source": [
        "###---------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdZa4d2H7nO8"
      },
      "source": [
        "###Adatbetöltés SZAVAK BETÖLTÉSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIfoz1647-Cq"
      },
      "source": [
        "words_url=\"https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/words.csv\""
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ8hnzVZ7iza",
        "outputId": "c7df1d2a-8d5c-4735-f966-ef9765a82eed"
      },
      "source": [
        "!wget $words_url"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-06 19:00:18--  https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/words.csv\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/words.csv [following]\n",
            "--2021-10-06 19:00:18--  https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/words.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 114745 (112K) [text/plain]\n",
            "Saving to: ‘words.csv’\n",
            "\n",
            "\rwords.csv             0%[                    ]       0  --.-KB/s               \rwords.csv           100%[===================>] 112.06K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-10-06 19:00:18 (35.5 MB/s) - ‘words.csv’ saved [114745/114745]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOoEemS17h2s"
      },
      "source": [
        "words=pd.read_csv(\"words.csv\",index_col=0)\n",
        "words.head()\n",
        "words_list=list(words[\"Words\"])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0UURG-L--8L"
      },
      "source": [
        "word_dict={v:i+1 for i,v in enumerate(words_list)}"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z94-mHYG_ISL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqbe8kFxgJav"
      },
      "source": [
        "##Tanulás"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsQObqfegZ7M"
      },
      "source": [
        "sentence.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHF2cpZxgh65"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOAaF9hwwhvp"
      },
      "source": [
        "df=pd.read_csv(\"TRAIN.csv\")"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuWeGPxRw4um"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roFT4HvqBQpa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCq2m69A39g3"
      },
      "source": [
        "table=list(df[\"sentence_code\"]) # typo\n",
        "lens=max([len(x) for x in table])\n",
        "sentiment=list(df[\"airline_sentiment\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGBReaGM6-zn"
      },
      "source": [
        "ytrain=[]\n",
        "for sent in sentiment:\n",
        "    if sent==\"negative\":\n",
        "        o=[1,0,0]\n",
        "    if sent==\"neutral\":\n",
        "        o=[0,1,0]\n",
        "    if sent==\"positive\":\n",
        "        o=[0,0,1]\n",
        "    ytrain.append(o)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNi-g2CNARwY"
      },
      "source": [
        "x0=[0 for _ in range(lens) ]\n",
        "xtrain=[]\n",
        "for sent in table:\n",
        "    o1=list(x0[0:lens-len(sent)]+list(sent))\n",
        "    xtrain.append(o1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRf3_wt9Bekb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JEpjcQSBWtx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41PgIpwlADTX"
      },
      "source": [
        "print(lens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59MTn_Fs3SvW"
      },
      "source": [
        "\n",
        "\n",
        "lstm_size=lens\n",
        "max_input_length=lens\n",
        "embedding_size=100\n",
        "n_words=len(table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvvPuVNSxNa7"
      },
      "source": [
        "\n",
        "# Importáld a megfelelő rétegeket\n",
        "from tensorflow.keras.layers import Input,Dense,Embedding,LSTM,TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta,Adam,SGD\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy,categorical_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "# Gondolj bele, hogy a hálóban az első réteg egy \"beágazás\" kell majd legyen\n",
        "# Ne feledd behozni a funkcionális vagy szekvenciális API-nak megfelelő \"fő\" osztályt\n",
        "# Adott esetben az optimalizálót\n",
        "# Nem külömben a \"bakcendet\", hogy jó gyakorlat szerint reseteld a gráfot\n",
        "#tf.reset_default_graph()\n",
        "K.clear_session\n",
        "x= Input(shape=(max_input_length,))\n",
        "embedded_x=Embedding(n_words+1,embedding_size, input_length=max_input_length-1, mask_zero=True)(x)\n",
        "lstm_output=LSTM(lstm_size,return_sequences=True)(embedded_x)\n",
        "lstm_output=LSTM(lstm_size,return_sequences=False)(lstm_output)\n",
        "predictions= Dense(3, activation=\"softmax\")(lstm_output)\n",
        "model=Model(inputs=x, outputs=predictions)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQpADS6gMHmd"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wutYc1k0HcL0"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzJYM8QeJyvI"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZSj92FGOebc"
      },
      "source": [
        "x_train=xtrain[0:-2000]\n",
        "y_train=ytrain[0:-2000]\n",
        "x_test=xtrain[-2000:]\n",
        "y_test=ytrain[-2000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACrVf-TbJYDe"
      },
      "source": [
        "train_x = np.asarray(xtrain)\n",
        "train_y = np.asarray(ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj_fAh_eKaSl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWIM4DP36S2L"
      },
      "source": [
        "# Loss \n",
        "\n",
        "loss = categorical_crossentropy # One-hot enkódolt kimenetünk van. Mit is használunk?\n",
        "\n",
        "# Optimizer\n",
        "optimizer = Adam() #Ízlés szerint...\n",
        " \n",
        "# Compilation\n",
        "#############\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op-I834SFpUX"
      },
      "source": [
        "# Illesszük az adatra a modellt. Használjunk 10% validációt\n",
        "history=model.fit(x=xtrain,y=ytrain,validation_data=( x_test,y_test), epochs=25, batch_size=100)\n",
        "# - nyelvmodellnél ez nem olyan lényeges\n",
        "# Használhatjuk a Keras beépített validációs splitjét.\n",
        "# Adjunk meg reális batch méretet!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_CHGgYja_bB"
      },
      "source": [
        "pred=model.predict(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIFRZ753bMNi"
      },
      "source": [
        "for i in range(len(pred)):\n",
        "    print(f\"{i}, {pred[i]}, {y_train[i]}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}