{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJ0ZL0ORvYUj1hUowJW7JC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/blob/20111008/Sentiment_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS8T-dxnwfjO"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDQLvAy5gI0C"
      },
      "source": [
        "##Szószedet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDUZMjUkgIIe",
        "outputId": "079de5c5-6b3b-420c-eb2e-bc0cfb5f76ff"
      },
      "source": [
        "!rm *zip*\n",
        "!wget https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
        "!unzip twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
        "df=pd.read_csv(\"TEST.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*zip*': No such file or directory\n",
            "--2021-10-07 17:08:21--  https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip [following]\n",
            "--2021-10-07 17:08:21--  https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1258171 (1.2M) [application/zip]\n",
            "Saving to: ‘twitter_sentiment_analysis_ai_challenge-dataset.zip’\n",
            "\n",
            "twitter_sentiment_a 100%[===================>]   1.20M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-10-07 17:08:21 (104 MB/s) - ‘twitter_sentiment_analysis_ai_challenge-dataset.zip’ saved [1258171/1258171]\n",
            "\n",
            "Archive:  twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
            "  inflating: SampleSubmission.csv    \n",
            "  inflating: TEST.csv                \n",
            "  inflating: TRAIN.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDtEcJB5haQF",
        "outputId": "2488cb61-b4d7-454c-b01c-8ed537dcb82a"
      },
      "source": [
        "df.head()\n",
        "alltext1=df.text\n",
        "print (len(alltext1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA8Mk878pUdU",
        "outputId": "b047e70c-ea2e-47c0-fad5-8f175810d4b2"
      },
      "source": [
        "df2=pd.read_csv(\"TRAIN.csv\")\n",
        "alltext2=df2.text\n",
        "print (len(alltext2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P-xoaRAp9gy",
        "outputId": "48346fef-c0fd-4909-9099-96e86b8f126d"
      },
      "source": [
        "alltext1=list(alltext1)\n",
        "alltext2=list(alltext2)\n",
        "alltext=alltext1+alltext2\n",
        "print(f\" {len(alltext)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 14640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OczaGqbTqKgz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vGSDdwGqneG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuRrm2HJja_6",
        "outputId": "172dea14-eb75-4a60-a0cf-900ad8cf0c2a"
      },
      "source": [
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "print(spell.correction(\"@frustrating\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "frustrating\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCN90l7Ijkb5"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_URL(text):\n",
        "    \"\"\"Remove URLs from a text string\"\"\"\n",
        "    return re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "def is_number(inp:str):\n",
        "    a=re.search(\"[\\d]\",inp)\n",
        "    if a==None: \n",
        "        return(False)\n",
        "    return(True)\n",
        "\n",
        "def strip_all_entities(text):\n",
        "    import re,string\n",
        "    entity_prefixes = ['“','@','#']\n",
        "    for separator in  string.punctuation:\n",
        "        if separator not in entity_prefixes :\n",
        "            text = text.replace(separator,' ')\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        word = word.strip()\n",
        "        if word:\n",
        "            if word[0] not in entity_prefixes:\n",
        "                words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def query_sentence(index):\n",
        "    print(f\"orig: {alltext[index]}\")\n",
        "    spell=SpellChecker()\n",
        "    inp=alltext[index]\n",
        "    inp=remove_URL(inp)\n",
        "    inp=remove_emoji(inp)\n",
        "    inp=strip_all_entities(inp)\n",
        "    \n",
        "    txt=inp.replace(\",\",\" \").replace(\".\",\" \").replace(\"!\",\" \").replace(\"#\",\" \").replace(\"???\",\" \").replace(\"??\",\" \").replace(\"?\",\" \").replace(\"  \",\" \").replace(\"  \",\" \").replace('\"',\"\").replace(\"“\",\"\")\n",
        "\n",
        "    #print(txt)\n",
        "    normal=\"\"\n",
        "    for i in txt.split(\" \"):\n",
        "        w1=i.lower().strip()\n",
        "        if w1 !=\"\":\n",
        "            w2=spell.correction(w1).strip()\n",
        "        else:\n",
        "            w2=\"\"\n",
        "        if w2==\"i\":\n",
        "            pass\n",
        "            #print(f\"chek: {w1}, {w2}\")\n",
        "        if is_number(w2):\n",
        "            w2=\"\"    \n",
        "        normal += w2+\" \"\n",
        "\n",
        "    print(f\"  conv: {normal}\")\n",
        "    return(normal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzMOTKbN2G3R"
      },
      "source": [
        "##Mentés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N9ZUtwPWj2wv",
        "outputId": "e84aec3b-abdb-4366-a7e1-6296f692b0ae"
      },
      "source": [
        "t=[]\n",
        "for i in range(len(alltext)):\n",
        "    print(f\"{i}\", end=\" \")\n",
        "    t.append(query_sentence(i))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 orig: @united well someone should tell that to the employees at the Denver baggage claim. Still no bag!!!!\n",
            "  conv: well someone should tell that to the employees at the denver baggage claim still no bag \n",
            "1 orig: @AmericanAir Let's all have a extraordinary week and make it a year to remember #GoingForGreat 2015 thanks so much American Airlines!!!\n",
            "  conv: let i all have a extraordinary week and make it a year to remember  thanks so much american airlines \n",
            "2 orig: @united since I have an international connection at 9am, I'm hoping for the same thing!\n",
            "  conv: since i have an international connection at am i i hoping for the same thing \n",
            "3 orig: @JetBlue woulda been nice of you to let us know that or post it!!! Your boards still show the flight on time!!!\n",
            "  conv: woulda been nice of you to let us know that or post it your boards still show the flight on time \n",
            "4 orig: @USAirways contd.. They put her on 7 pm flite tonite. I think she's on now. Worst customer service ever! U need to fix it.\n",
            "  conv: cond they put her on  am elite tonite i think she i on now worst customer service ever u need to fix it \n",
            "5 orig: @JetBlue saving my sanity. Leaving it behind for sunshine. #escape #FL #bliss #travel #InDenial #WhatFrozenPipes http://t.co/6TtzEJV3hY\n",
            "  conv: saving my sanity leaving it behind for sunshine \n",
            "6 orig: @AmericanAir WORST SERVICE EVER!! Delayed flights for more than 5 hours plus you missed my bag! And your employees are rude 😡😡\n",
            "  conv: worst service ever delayed flights for more than  hours plus you missed my bag and your employees are rude \n",
            "7 orig: @USAirways, I'm expecting 400% reimbursement for the unprofessional decisions with US728 on 21 Feb. Check your very full complaint box.\n",
            "  conv: i i expecting  reimbursement for the unprofessional decisions with  on  feb check your very full complaint box \n",
            "8 orig: @AmericanAir I might look into that. My wife travels much more than I do. Could we both use the membership?\n",
            "  conv: i might look into that my wife travels much more than i do could we both use the membership \n",
            "9 orig: @JetBlue I thought being a mosaic member had 'perks' the best part is instead of checking the extra 5lbs I'm carrying it on 😒\n",
            "  conv: i thought being a mosaic member had perks the best part is instead of checking the extra plus i i carrying it on \n",
            "10 orig: @AmericanAir is there a probability that flight 1330 from DFW to DTW will be delayed or Cancelled Flighted?\n",
            "  conv: is there a probability that flight  from dew to dew will be delayed or cancelled lighted \n",
            "11 orig: @USAirways we already spoke to someone several times about the matter and no one is sympathetic or will fly us home early complimentary.\n",
            "  conv: we already spoke to someone several times about the matter and no one is sympathetic or will fly us home early complimentary \n",
            "12 orig: @united I am still in the airport waiting for my flight.\n",
            "  conv: i am still in the airport waiting for my flight \n",
            "13 orig: @USAirways you suck and you ruined my day. And my vacation isn't even until June.\n",
            "  conv: you suck and you ruined my day and my vacation isn i even until june \n",
            "14 orig: @AmericanAir please dm me\n",
            "  conv: please do me \n",
            "15 orig: @united can we get an explanation on why UA978 from São Paulo to Houston has been delayed? Now we're hearing from crew about Cancelled Flightlations.\n",
            "  conv: can we get an explanation on why  from so paulo to houston has been delayed now we re hearing from crew about cancelled flightlations \n",
            "16 orig: @SouthwestAir is there a way to know who checked my bag on the curb? She was awesome!!! And want to be sure she gets a high five!\n",
            "  conv: is there a way to know who checked my bag on the curb she was awesome and want to be sure she gets a high five \n",
            "17 orig: @united Your \"Loyalty Team\" basically flipped me off via phone, but thanks. Maybe Google \"loyalty\" and get back to me? ^LOL\n",
            "  conv: your loyalty team basically flipped me off via phone but thanks maybe google loyalty and get back to me lol \n",
            "18 orig: @USAirways  #USAirways disappoints AGAIN! #Cancelled Flighted flights #Missed appointments #Promised refund is a lie\n",
            "  conv: disappoints again lighted flights appointments refund is a lie \n",
            "19 orig: @USAirways I have fam thats supposed to fly 2/22 that are currently in the hosp.3 trys to speak to a human, yr phone sys cuts off. pls hlp!\n",
            "  conv: i have fam thats supposed to fly   that are currently in the hosp  trys to speak to a human yr phone sys cuts off plus help \n",
            "20 orig: @AmericanAir @MattThomasNews When did American Airlines start flying to and from Siberia? Wait, this is the USA? Oy!\n",
            "  conv: when did american airlines start flying to and from siberia wait this is the usa oy \n",
            "21 orig: @AmericanAir Thanks for your canned response that makes it look like you care about your customers. I'm sure all Twitter users fall for it..\n",
            "  conv: thanks for your canned response that makes it look like you care about your customers i i sure all twitter users fall for it \n",
            "22 orig: @united Just got demoted from Gold cuz my hubby got 100% of the PQD, tho there were enuf $ spent for 4 Golds.#spousal.discrimination/angry.\n",
            "  conv: just got demoted from gold cuz my hubby got  of the pod tho there were neuf spent for  golds discrimination angry \n",
            "23 orig: @united the lounge tells us they have no pillows for my grandma as one of the ladies opens the closet and I see 2 right there. #unitedlies\n",
            "  conv: the lounge tells us they have no pillows for my grandma as one of the ladies opens the closet and i see  right there \n",
            "24 orig: @AmericanAir robocalls me with another Cancelled Flightation. And then when I don’t accept the change it won’t let me connect to an agent. Just wow.\n",
            "  conv: robocall me with another cancelled flirtation and then when i don't accept the change it won't let me connect to an agent just wow \n",
            "25 orig: .@AmericanAir Hopefully it's all fixed. They've got a new aircraft for us- just waiting to board.\n",
            "  conv: hopefully it i all fixed they ve got a new aircraft for us just waiting to board \n",
            "26 orig: @SouthwestAir yea but I was hoping to avoid the ticket difference amount as a birthday courtesy? #flysouthwest #lovesouthwest #mybday\n",
            "  conv: yea but i was hoping to avoid the ticket difference amount as a birthday courtesy \n",
            "27 orig: @SouthwestAir while you clearly didn't care about our troubles yday thought I'd share bags took &gt;90min and came back absolutely drenched..\n",
            "  conv: while you clearly didn i care about our troubles day thought i i share bags took it min and came back absolutely drenched \n",
            "28 orig: @USAirways 4 hours . How do I change my flight that was Cancelled Flightled ?\n",
            "  conv:  hours how do i change my flight that was cancelled lighted \n",
            "29 orig: @SouthwestAir lol I already am ! I am a card member as well too lol i enjoy flying with you Guys\n",
            "  conv: lol i already am i am a card member as well too lol i enjoy flying with you guys \n",
            "30 orig: @united very poor customer service. I WILL think again befor Flight Booking Problems another United flight.\n",
            "  conv: very poor customer service i will think again befor flight booking problems another united flight \n",
            "31 orig: @SouthwestAir I love you guys! Had to take a few other airlines this week...makes me love and appreciate y'all so much more! #onlywaytofly\n",
            "  conv: i love you guys had to take a few other airlines this week makes me love and appreciate y all so much more \n",
            "32 orig: @AmericanAir and just bad cs! I will be back on @JetBlue  at least when you stuck they look out for you and apologize for any trouble\n",
            "  conv: and just bad is i will be back on at least when you stuck they look out for you and apologize for any trouble \n",
            "33 orig: @VirginAmerica to begin Dallas-Austin #flights in April - 88.9 KETR http://t.co/SSUVWwkyHH\n",
            "  conv: to begin dallas austin in april   kerr \n",
            "34 orig: “@JetBlue: Our fleet's on fleek. http://t.co/Vxn2J36M7V” this happened\n",
            "  conv: our fleet i on fleek this happened \n",
            "35 orig: @united Worked like a charm. Bag was waiting on the carousel when we got to baggage claim. #welldone #goodflight #friendlysky\n",
            "  conv: worked like a charm bag was waiting on the carousel when we got to baggage claim \n",
            "36 orig: @AmericanAir thanks!\n",
            "  conv: thanks \n",
            "37 orig: @united thanks! Will you guys be getting the A380s anytime soon?\n",
            "  conv: thanks will you guys be getting the  anytime soon \n",
            "38 orig: @AmericanAir your call center won't let me wait on hold, which I would happily do. Am I seriously supposed to just keep calling? Not great\n",
            "  conv: your call center won i let me wait on hold which i would happily do am i seriously supposed to just keep calling not great \n",
            "39 orig: @USAirways Today USAir Cancelled Flightled our rescheduled flight &amp; did not notify us except possibly to our home phone-not helpful since we are in CO\n",
            "  conv: today usain cancelled lighted our rescheduled flight amp did not notify us except possibly to our home phone not helpful since we are in co \n",
            "40 orig: @SouthwestAir Great flight yesterday from MSY to AUS!! Thank you for such great safety,service and beautiful skies!! http://t.co/X1EqYAHfvZ\n",
            "  conv: great flight yesterday from my to aus thank you for such great safety service and beautiful skies \n",
            "41 orig: @USAirways over the phone. I called the 6170 number and she picked up almost immediately.\n",
            "  conv: over the phone i called the  number and she picked up almost immediately \n",
            "42 orig: @JetBlue Did you discontinue nonstop service from SJC to BOS? Can't find 471 or 472 anywhere\n",
            "  conv: did you discontinue nonstop service from sec to bos can i find  or  anywhere \n",
            "43 orig: @USAirways thanks\n",
            "  conv: thanks \n",
            "44 orig: @JetBlue as requested, here's selfie somewhere over the Rocky's. #milehighselfieclub http://t.co/A1ChOxKPJp\n",
            "  conv: as requested here i selfie somewhere over the rocky i \n",
            "45 orig: @AmericanAir I have \"continued contacting\" your phone reps for 7 hours and am still unable to get through. That's absurd and I am livid.\n",
            "  conv: i have continued contacting your phone reps for  hours and am still unable to get through that i absurd and i am livid \n",
            "46 orig: @SouthwestAir @Imaginedragons #DestinationDragons Scavenger Hunt rules: http://t.co/vHgkiTzSaw\n",
            "  conv: scavenger hunt rules \n",
            "47 orig: @USAirways pilot for flight 729 didn't show until after departure ⌚️ &amp; now there's a broken computer. Next time flying @United. #ALWAYSLate Flight\n",
            "  conv: pilot for flight  didn i show until after departure i amp now there i a broken computer next time flying flight \n",
            "48 orig: @USAirways passengers sitting on plane for two hours flight #4663 from CMH!!!  All other flights have left #usairwaysfail #worstairlineever\n",
            "  conv: passengers sitting on plane for two hours flight from cme all other flights have left \n",
            "49 orig: “@JetBlue: @sylvie75015 Good morning, Sylvie! Have a great flight! #yourock” &gt; Thank you #JetBlue! @mxo42 @henrikwagner73 #JetBlueRocks\n",
            "  conv: good morning sylvie have a great flight it thank you \n",
            "50 orig: @SouthwestAir Just go ahead and start the scavenger hunt after 5 pm today when work is over ;) #DestinationDragons\n",
            "  conv: just go ahead and start the scavenger hunt after  am today when work is over \n",
            "51 orig: @united  your costumer service today in the Providence airport was sucked. Recommend your airline learn to check the weather and be honest\n",
            "  conv: your costumer service today in the providence airport was sucked recommend your airline learn to check the weather and be honest \n",
            "52 orig: @AmericanAir  beyond frustrated with no call back from auto hold or whatever you call it. Entered my number at 11:30 CST, still no call 2:26\n",
            "  conv: beyond frustrated with no call back from auto hold or whatever you call it entered my number at   cut still no call   \n",
            "53 orig: @JetBlue got it. thanks the quick reply.\n",
            "  conv: got it thanks the quick reply \n",
            "54 orig: @AmericanAir @British_Airways trying to speak with an agent about my flight to London tonight but can't get anyone from AA.  Can you help?\n",
            "  conv: airways trying to speak with an agent about my flight to london tonight but can i get anyone from a can you help \n",
            "55 orig: @SouthwestAir what robot is running this account. The same one that doesn't remember anything they ask. Same conversation over and over.\n",
            "  conv: what robot is running this account the same one that doesn i remember anything they ask same conversation over and over \n",
            "56 orig: @AmericanAir Here you go https://t.co/oM1vIEg74a\n",
            "  conv: here you go \n",
            "57 orig: @united ROC flight Cancelled Flightled. Love my snowy drive home with no useful substitutes\n",
            "  conv: roc flight cancelled lighted love my snowy drive home with no useful substitutes \n",
            "58 orig: @USAirways flt 1820 2rsw could have extended the courtsey of waiting 2min 4 10 of us.... gates closed in r faces.  Ur fired\n",
            "  conv: felt  grow could have extended the courtesy of waiting min   of us gates closed in i faces ur fired \n",
            "59 orig: Spend 1 HOUR on hold with @USAirways .\n",
            "  conv: spend  hour on hold with \n",
            "60 orig: @JetBlue could I get a free flight to Vegas since it's my bday😏☺️\n",
            "  conv: could i get a free flight to vegas since it i my day \n",
            "61 orig: @united thank you for listening to my compliant and doing the right thing. I appreciate you working with me\n",
            "  conv: thank you for listening to my compliant and doing the right thing i appreciate you working with me \n",
            "62 orig: Thank you for your help, Shannon! Great customer service, @SouthwestAir.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-175-8751644592d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malltext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-174-64d3b917d7b5>\u001b[0m in \u001b[0;36mquery_sentence\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"orig: {alltext[index]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mspell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSpellChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malltext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_URL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, local_dictionary, distance, tokenizer, case_sensitive)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mlang_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_open\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_frequency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivPXsq3JoP0m"
      },
      "source": [
        "s=set()\n",
        "for sent in t:\n",
        "    wlist=sent.strip().split()\n",
        "    for w in wlist:\n",
        "        s.add(w)\n",
        "print (len(s))\n",
        "ordered=list(s)\n",
        "ordered.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi9MDCpq6bJv"
      },
      "source": [
        "print(ordered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIoSRwse2GOY"
      },
      "source": [
        "sentence=pd.DataFrame(t)\n",
        "sentence.columns=[\"Sentence\"]\n",
        "sentence.to_csv(\"Sentences.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbDPRdUc6gnp"
      },
      "source": [
        "wordlist=pd.DataFrame(ordered)\n",
        "wordlist.columns=[\"Words\"]\n",
        "wordlist.to_csv(\"words.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RCD1OKy7kJr"
      },
      "source": [
        "###---------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdZa4d2H7nO8"
      },
      "source": [
        "###Adatbetöltés SZAVAK BETÖLTÉSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7heTRRxuS2U"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIfoz1647-Cq"
      },
      "source": [
        "words_url=\"https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/words.csv\""
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ8hnzVZ7iza",
        "outputId": "400821c8-e7b9-471d-885a-8ad37efe494a"
      },
      "source": [
        "!wget $words_url"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-08 13:46:18--  https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/words.csv\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/words.csv [following]\n",
            "--2021-10-08 13:46:18--  https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/words.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 114745 (112K) [text/plain]\n",
            "Saving to: ‘words.csv.3’\n",
            "\n",
            "words.csv.3         100%[===================>] 112.06K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-10-08 13:46:18 (5.26 MB/s) - ‘words.csv.3’ saved [114745/114745]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOoEemS17h2s"
      },
      "source": [
        "words=pd.read_csv(\"words.csv\",index_col=0)\n",
        "words.head()\n",
        "words_list=list(words[\"Words\"])"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0UURG-L--8L"
      },
      "source": [
        "word_dict={v:i+1 for i,v in enumerate(words_list)}"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqnFYKaeCkda"
      },
      "source": [
        "### adatbetöltés mondatok betöltése"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro6DsEKaCiwF"
      },
      "source": [
        "sentences_url=\"https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/Sentences.csv\""
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z94-mHYG_ISL",
        "outputId": "af68a115-8465-48df-9baa-703651e35cfe"
      },
      "source": [
        "!wget $sentences_url\n",
        "sentences=pd.read_csv(\"Sentences.csv\",index_col=0)\n",
        "sentences.head()\n",
        "sentences_list=list(sentences[\"Sentence\"])"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-08 13:46:18--  https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/Sentences.csv\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/Sentences.csv [following]\n",
            "--2021-10-08 13:46:19--  https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/bc5a87c1da7a3fdcd674325ca8d1ef417075fbd0/Sentences.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1313702 (1.3M) [text/plain]\n",
            "Saving to: ‘Sentences.csv.3’\n",
            "\n",
            "Sentences.csv.3     100%[===================>]   1.25M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-10-08 13:46:19 (22.8 MB/s) - ‘Sentences.csv.3’ saved [1313702/1313702]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsQObqfegZ7M"
      },
      "source": [
        ""
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqbe8kFxgJav"
      },
      "source": [
        "##Tanulás"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHF2cpZxgh65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8f8fde-cc20-4724-8cd0-a4c1df7e5c1e"
      },
      "source": [
        "!rm *zip*\n",
        "!rm TEST*\n",
        "!rm TRAIN*\n",
        "!rm SampleSubmission*\n",
        "!wget https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
        "!unzip twitter_sentiment_analysis_ai_challenge-dataset.zip\n"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-08 14:15:18--  https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip [following]\n",
            "--2021-10-08 14:15:18--  https://raw.githubusercontent.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/9674d3c8ef760a8d2c5cc2e3af8a71faa93edf7d/twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1258171 (1.2M) [application/zip]\n",
            "Saving to: ‘twitter_sentiment_analysis_ai_challenge-dataset.zip’\n",
            "\n",
            "twitter_sentiment_a 100%[===================>]   1.20M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-10-08 14:15:18 (21.0 MB/s) - ‘twitter_sentiment_analysis_ai_challenge-dataset.zip’ saved [1258171/1258171]\n",
            "\n",
            "Archive:  twitter_sentiment_analysis_ai_challenge-dataset.zip\n",
            "replace SampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: SampleSubmission.csv    \n",
            "  inflating: TEST.csv                \n",
            "  inflating: TRAIN.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOAaF9hwwhvp"
      },
      "source": [
        "df=pd.read_csv(\"TRAIN.csv\")"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuWeGPxRw4um"
      },
      "source": [
        "df[\"clean_text\"]=sentences_list[2640:]"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roFT4HvqBQpa"
      },
      "source": [
        "all_sent=[]\n",
        "for sentence in df.clean_text:\n",
        "    sent_list=sentence.strip().split()\n",
        "    out=[]\n",
        "    for word in sent_list:\n",
        "        out.append(word_dict[word]) \n",
        "    all_sent.append(out)"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrZFC5BAEb2k"
      },
      "source": [
        "df[\"sentence_code\"]=all_sent"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCq2m69A39g3"
      },
      "source": [
        "table=list(df[\"sentence_code\"]) # typo\n",
        "lens=max([len(x) for x in table])\n",
        "sentiment=list(df[\"airline_sentiment\"])\n"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGBReaGM6-zn"
      },
      "source": [
        "ytrain=[]\n",
        "for sent in sentiment:\n",
        "    if sent==\"negative\":\n",
        "        o=[1,0]\n",
        "    if sent==\"neutral\":\n",
        "        o=[0.5,0.5]\n",
        "    if sent==\"positive\":\n",
        "        o=[0,1]\n",
        "    ytrain.append(o)\n",
        "    "
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNi-g2CNARwY"
      },
      "source": [
        "x0=[0 for _ in range(lens) ]\n",
        "xtrain=[]\n",
        "for sent in table:\n",
        "    o1=list(x0[0:lens-len(sent)]+list(sent))\n",
        "    xtrain.append(o1)"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRf3_wt9Bekb"
      },
      "source": [
        ""
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JEpjcQSBWtx"
      },
      "source": [
        ""
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41PgIpwlADTX",
        "outputId": "85bbecec-a9e7-496e-dd4d-2dfc78a403a6"
      },
      "source": [
        "print(lens)"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59MTn_Fs3SvW"
      },
      "source": [
        "\n",
        "\n",
        "lstm_size=lens\n",
        "max_input_length=lens\n",
        "embedding_size=150 #(100: 73%)\n",
        "n_words=len(table)"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvvPuVNSxNa7"
      },
      "source": [
        "\n",
        "# Importáld a megfelelő rétegeket\n",
        "from tensorflow.keras.layers import Input,Dense,Embedding,LSTM,TimeDistributed, Flatten, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta,Adam,SGD\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy,categorical_crossentropy, mean_squared_error\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "K.clear_session\n",
        "x= Input(shape=(max_input_length,))\n",
        "embedded_x=Embedding(n_words+1,embedding_size, input_length=max_input_length-1, mask_zero=True)(x)\n",
        "lstm_output=Bidirectional( LSTM(lstm_size,return_sequences=True))(embedded_x)\n",
        "lstm_output=LSTM(lstm_size,return_sequences=True)(lstm_output)\n",
        "\n",
        "\n",
        "lstm_output=Flatten()(lstm_output)\n",
        "#Dense_out= Dense(33, activation=\"softmax\")(lstm_output)\n",
        "\n",
        "predictions= Dense(2, activation=\"softmax\")(lstm_output)\n",
        "model=Model(inputs=x, outputs=predictions)\n",
        "\n"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQpADS6gMHmd",
        "outputId": "7edd2da5-1555-4726-f4fa-ffdf1476392f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 33)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_8 (Embedding)      (None, 33, 150)           1800150   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 33, 66)            48576     \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 33, 33)            13200     \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 1089)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 2)                 2180      \n",
            "=================================================================\n",
            "Total params: 1,864,106\n",
            "Trainable params: 1,864,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wutYc1k0HcL0",
        "outputId": "113f2a67-5e2d-47e1-a90a-73ce36f7de79"
      },
      "source": [
        "x"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 33) dtype=float32 (created by layer 'input_9')>"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzJYM8QeJyvI"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZSj92FGOebc"
      },
      "source": [
        "x_train=xtrain[0:-2000]\n",
        "y_train=ytrain[0:-2000]\n",
        "x_test=xtrain[-2000:]\n",
        "y_test=ytrain[-2000:]"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACrVf-TbJYDe"
      },
      "source": [
        "train_x = np.asarray(xtrain)\n",
        "train_y = np.asarray(ytrain)"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj_fAh_eKaSl"
      },
      "source": [
        ""
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWIM4DP36S2L"
      },
      "source": [
        "# Loss \n",
        "\n",
        "loss = mean_squared_error #categorical_crossentropy # One-hot enkódolt kimenetünk van. Mit is használunk?\n",
        "\n",
        "# Optimizer\n",
        "optimizer = Adam() #Ízlés szerint...\n",
        " \n",
        "# Compilation\n",
        "#############\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=loss)"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op-I834SFpUX",
        "outputId": "59638fde-e94b-445a-cbc2-09137a2bddde"
      },
      "source": [
        "# Illesszük az adatra a modellt. Használjunk 10% validációt\n",
        "history=model.fit(x=xtrain,y=ytrain,validation_data=( x_test,y_test), epochs=50, batch_size=100,)\n",
        "# - nyelvmodellnél ez nem olyan lényeges\n",
        "# Használhatjuk a Keras beépített validációs splitjét.\n",
        "# Adjunk meg reális batch méretet!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "120/120 [==============================] - 26s 146ms/step - loss: 0.0979 - val_loss: 0.0463\n",
            "Epoch 2/50\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.0517 - val_loss: 0.0323\n",
            "Epoch 3/50\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.0368 - val_loss: 0.0246\n",
            "Epoch 4/50\n",
            "112/120 [===========================>..] - ETA: 0s - loss: 0.0293"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_CHGgYja_bB"
      },
      "source": [
        "pred=model.predict(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIFRZ753bMNi"
      },
      "source": [
        "for i in range(len(pred)):\n",
        "    print(f\"{i}, {pred[i]}, {y_train[i]}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlKyrRqfG6GP"
      },
      "source": [
        "### Model usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvmmKvBIF4n8"
      },
      "source": [
        "test_df=pd.read_csv(\"TEST.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc0vyTmcHRvY"
      },
      "source": [
        "print(len(test_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6y9O5tnHq9Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5utaqxGNHU7y"
      },
      "source": [
        "test_df[\"clean_text\"]=sentences_list[:2640]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_km2bK90Hydk"
      },
      "source": [
        "def code_sentences(df):\n",
        "    all_sent=[]\n",
        "    for sentence in df.clean_text:\n",
        "        sent_list=sentence.strip().split()\n",
        "        out=[]\n",
        "        for word in sent_list:\n",
        "            out.append(word_dict[word]) \n",
        "        all_sent.append(out)\n",
        "    df[\"sentence_code\"]=all_sent\n",
        "    return (df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABUxosxoIkmI"
      },
      "source": [
        "test_df=code_sentences(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlMCct35I5Oe"
      },
      "source": [
        "def push_sentences(df,lens=33):\n",
        "    table=list(df[\"sentence_code\"]) # typo\n",
        "    \n",
        "    print(lens)\n",
        "    x0=[0 for _ in range(lens) ]\n",
        "    x=[]\n",
        "    for sent in table:\n",
        "        o1=list(x0[0:lens-len(sent)]+list(sent))\n",
        "        x.append(o1)\n",
        "    return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhu_bX9mKx-i"
      },
      "source": [
        "xx=push_sentences(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bZFjo4TLE9R"
      },
      "source": [
        "test_out=model.predict(xx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lokLXV-uLs4-"
      },
      "source": [
        "test_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LotAte_L8c9"
      },
      "source": [
        "def decode3(model_out):\n",
        "    y=[]\n",
        "    for mo in model_out:\n",
        "        if mo[0]>mo[1] and mo[0]>mo[2]:\n",
        "            o=\"negative\"    \n",
        "        if mo[1]>mo[0] and mo[1]>mo[2]:\n",
        "            o=\"neutral\"    \n",
        "        if mo[2]>mo[0] and mo[2]>mo[1]:\n",
        "            o=\"positive\"    \n",
        "        y.append(o)\n",
        "    return(y)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qf1AhjuObA-"
      },
      "source": [
        "def decode(model_out):\n",
        "    y=[]\n",
        "    for mo in model_out:\n",
        "        dif=abs(mo[0]-mo[1])\n",
        "        if dif < 0.2 :\n",
        "            o=\"neutral\"\n",
        "        else:\n",
        "            if mo[0]>mo[1] :\n",
        "                o=\"negative\"    \n",
        "            else:\n",
        "                o=\"positive\"    \n",
        "        y.append(o)\n",
        "    return(y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1vU8pV0NDLx"
      },
      "source": [
        "prediction=decode(test_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV7YM6GBYEYX"
      },
      "source": [
        "test_df[\"predicted\"]=prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aERaT3-tNNMs"
      },
      "source": [
        "prediction_df=pd.DataFrame(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Y7T_cxNjar"
      },
      "source": [
        "prediction_df.columns=[\"Sentiment\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5SaPlp4OAWx"
      },
      "source": [
        "prediction_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OnUZvarOKWm"
      },
      "source": [
        "prediction_df.index.name=\"Index\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw4CUPobbH7A"
      },
      "source": [
        "from datetime  import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAbhqmcPdF6U"
      },
      "source": [
        "a=datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiOwO9UTdeUt"
      },
      "source": [
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ELciPC_OpWA"
      },
      "source": [
        "prediction_df.to_csv(\"submission_\"+a+\".csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvmaQZGBYM_7"
      },
      "source": [
        "predcsv=test_df.loc[:,[\"text\",\"predicted\"]]\n",
        "\n",
        "predcsv.to_csv(\"mini.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrtPBTgcYwtg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}