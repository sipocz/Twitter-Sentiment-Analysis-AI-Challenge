{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_LM_Task_edited.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/blob/main/LSTM_LM_Task_edited.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R04Si0M8KzK"
      },
      "source": [
        "# LSTM nyelvmodell\n",
        "\n",
        "Az inspiráció nem más, mint  [Andrej Karpathy híres írása](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
        "\n",
        "\n",
        "## Adatbeolvasás"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-10-06T16:48:43.620252Z",
          "start_time": "2018-10-06T16:48:43.607851Z"
        },
        "id": "egfH6gkf8WQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a008289-7a75-4944-97f3-e1aa10cc7577"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1212)\n",
        "\n",
        "#tf.random.set_random_seed(1234)\n",
        "\n",
        "nltk.download(\"brown\")\n",
        "\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# This can be an important parameter, so be aware of it...\n",
        "max_seq_length = 15\n",
        "max_num_of_sents = 57200\n",
        "# max_num_of_sents = 50 # How many sentences should we read from the corpus (max=57200)\n",
        "\n",
        "def generate_brown_word_to_id_map():\n",
        "    \"\"\"Return a dictionary mapping downcased Brown-words to their ids.\n",
        "    Numbering starts from 1 since we use 0 for masking (!!!).\n",
        "    \"\"\"\n",
        "    words = set()\n",
        "    for word in brown.words():\n",
        "        words.add(word.lower())\n",
        "    return {word: idx + 1 for idx, word in enumerate(sorted(words))}\n",
        "\n",
        "\n",
        "class BrownReader:\n",
        "    \"\"\"A reader class for the Brown corpus.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_to_id_map = generate_brown_word_to_id_map()\n",
        "        self.id_to_word_map = {idx: word for word, idx in self.word_to_id_map.items()}\n",
        "\n",
        "    def n_words(self):\n",
        "        return len(self.word_to_id_map)\n",
        "\n",
        "    def sentence_to_ids(self, sentence):\n",
        "        \"\"\"Return the word ids of a sentence.\n",
        "        \"\"\"\n",
        "        return [self.word_to_id_map[word.lower()] for word in sentence]\n",
        "        \n",
        "    def sentences(self):\n",
        "        \"\"\"Generator yielding features from the Brown corpus.\n",
        "        \"\"\"\n",
        "        return (self.sentence_to_ids(sentence) for sentence in brown.sents())\n",
        "\n",
        "    def sentence_matrixes(self):\n",
        "        x = np.zeros((max_num_of_sents, max_seq_length-1))\n",
        "        y = np.zeros((max_num_of_sents, max_seq_length-1))\n",
        "        sents = self.sentences()\n",
        "        for idx, sent in enumerate(sents):\n",
        "            if idx == max_num_of_sents:\n",
        "                break\n",
        "            np_array = np.asarray(sent)\n",
        "            length  = min(max_seq_length, len(np_array))\n",
        "            x[idx, :length - 1] = np_array[:length - 1]\n",
        "            y[idx, :length - 1] = np_array[1:length]\n",
        "        return x, y\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvjdF2_p9_YV"
      },
      "source": [
        "## Modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SavNwc5m9_Ya"
      },
      "source": [
        "### Paraméterek"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXHNzCXj9_Ye"
      },
      "source": [
        "br = BrownReader()\n",
        "n_words = br.n_words()\n",
        "\n",
        "max_input_length = max_seq_length - 1 # since our x/y input does not contain the last/first element of the sentences"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkhDDIwl9_ZC"
      },
      "source": [
        "data_x, data_y = br.sentence_matrixes()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJdzjP6R9_ZK"
      },
      "source": [
        "data_y = np.expand_dims(data_y, -1) # It seems that Keras needs this for the \"one-cold\" and softmax dims to match"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-FkvXvToZim"
      },
      "source": [
        "# Feladatok\n",
        "\n",
        "Alább"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap0pGV4IoZin"
      },
      "source": [
        "# háló paraméterek\n",
        "\n",
        "lstm_size = 512\n",
        "embedding_size = 150\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWPOCc6q9_Yn"
      },
      "source": [
        "### Háló"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXth0CDm9_Yq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e39b25d-debd-4225-8fd0-4293cf187247"
      },
      "source": [
        "# Import:\n",
        "\n",
        "# Importáld a megfelelő rétegeket\n",
        "from tensorflow.keras.layers import Input,Dense,Embedding,LSTM,TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta,Adam,SGD\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "# Gondolj bele, hogy a hálóban az első réteg egy \"beágazás\" kell majd legyen\n",
        "# Ne feledd behozni a funkcionális vagy szekvenciális API-nak megfelelő \"fő\" osztályt\n",
        "# Adott esetben az optimalizálót\n",
        "# Nem külömben a \"bakcendet\", hogy jó gyakorlat szerint reseteld a gráfot\n",
        "#tf.reset_default_graph()\n",
        "K.clear_session\n",
        "x= Input(shape=(max_input_length,))\n",
        "embedded_x=Embedding(n_words+1,embedding_size, input_length=max_input_length-1, mask_zero=True)(x)\n",
        "lstm_output=LSTM(lstm_size,return_sequences=True)(embedded_x)\n",
        "lstm_output=LSTM(lstm_size,return_sequences=True)(lstm_output)\n",
        "predictions= Dense(n_words+1, activation=\"softmax\")(lstm_output)\n",
        "model=Model(inputs=x, outputs=predictions)\n",
        "\n",
        "\n",
        "\n",
        "# valmint a ritka, kategorikus keresztentrópiát, mint veszteségfüggvényt\n",
        "\n",
        "# jó gyakorlat: reseteld a gráfot!\n",
        "\n",
        "# Model\n",
        "########\n",
        "# Építs modellt!\n",
        "# Bemeneti réteggel kezdd, aminek az inputja egy maximális szöveghosszt jelző vektor. \n",
        "# Meg ugye hogy akár vektor, akár nem, a shape az tuple...\n",
        "# Ezt kövesse egy beágyazás réteg.\n",
        "# FIGYELEM: \n",
        "# 1. a szélessége a szavak száma plusz 1\n",
        "# 2. mérete paraméterben adott, lásd fent\n",
        "# 3. bemenet hossza: legnagyobb bemenet mérete -1\n",
        "# 4. a nulla értékek maszkolandóak benne\n",
        "# FELADAT: Ezeket indokold meg, miért?\n",
        "\n",
        "# Következő KÉT rétegben LSTM-ek legyenek.\n",
        "# Ugye ahhoz, hogy egymásra építhetőek legyenek, nem csak a szekvencia végi predikcióikat kell visszaadják\n",
        "# Erre van valahol egy csinos paraméter... ;-)\n",
        "\n",
        "# Végül pedig egy fully connected layerrel és softmaxxal projektáljuk a kimenetet.\n",
        "# Mennyi a szélessége? (Segítség: ha a fentieket jól megindokoltad, akkor már tudod. ;-)\n",
        "\n",
        "# Végül példányosítsuk a modellt!\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 14)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 14, 150)           7472400   \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 14, 512)           1357824   \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 14, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 14, 49816)         25555608  \n",
            "=================================================================\n",
            "Total params: 36,485,032\n",
            "Trainable params: 36,485,032\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKXuebno9_Y1"
      },
      "source": [
        "### Hiba, optimalizáció és modelfordítás"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mob9-6s9_Y3"
      },
      "source": [
        "# Loss \n",
        "\n",
        "loss = sparse_categorical_crossentropy # One-hot enkódolt kimenetünk van. Mit is használunk?\n",
        "\n",
        "# Optimizer\n",
        "optimizer = Adam() #Ízlés szerint...\n",
        " \n",
        "# Compilation\n",
        "#############\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=loss)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLLqU3c_9_Y8"
      },
      "source": [
        "### Tréning\n",
        "\n",
        "Előállítjuk a tréningadatot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhsD6dnHQdWr"
      },
      "source": [
        "data_x, data_y = br.sentence_matrixes()\n",
        "data_y=np.expand_dims(data_y,-1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMPFHd-M9_ZX"
      },
      "source": [
        "És trénelünk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJboxaY_9_Zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea3edd27-2f20-49ff-b45c-5ec513b305a5"
      },
      "source": [
        "# Illesszük az adatra a modellt. Használjunk 10% validációt\n",
        "history=model.fit(x=data_x,y=data_y,validation_split=0.1, epochs=5, batch_size=100)\n",
        "# - nyelvmodellnél ez nem olyan lényeges\n",
        "# Használhatjuk a Keras beépített validációs splitjét.\n",
        "# Adjunk meg reális batch méretet!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "172/515 [=========>....................] - ETA: 30:39 - loss: 6.6941"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eao03t2U9gBy"
      },
      "source": [
        "## Demó 1: Következő szó"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-10-06T17:04:58.584741Z",
          "start_time": "2018-10-06T16:48:43.630Z"
        },
        "id": "RM1WVrua9qaY",
        "scrolled": true
      },
      "source": [
        "# Prediction\n",
        "############\n",
        "\n",
        "def str_to_input(s):\n",
        "    \"\"\"Convert a string to appropriate model input.\n",
        "    \"\"\"\n",
        "    words = [x.lower() for x in s.split()[:max_input_length]]\n",
        "    ids = [br.word_to_id_map[word] for word in words]\n",
        "    ids_array = np.asarray(ids)\n",
        "    length = min(max_input_length, len(ids_array))\n",
        "    result = np.zeros((1, max_input_length))\n",
        "    result[0, :length] = ids_array[:length]\n",
        "    return result, length\n",
        "    \n",
        "\n",
        "while True:\n",
        "    s = input(\"\\nEnter a few starting words of a sentence or <return> to stop: \")\n",
        "    if s == \"\":\n",
        "        break\n",
        "    else:\n",
        "        try:\n",
        "            x, length = str_to_input(s)\n",
        "            predictions = model.predict(x)\n",
        "            probs = predictions[0][length - 1]\n",
        "            most_probable = np.argmax(probs)\n",
        "            print(\"Predicted next word:\", br.id_to_word_map[most_probable])\n",
        "        except KeyError:\n",
        "            print(\"Unknown words -- please try again!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4AkEJxBzPfm"
      },
      "source": [
        "## Demó 2: Mondatok hasonlósága\n",
        "\n",
        "Először egy függvényt definiálunk, amely előállítja a rejtett LSTM állapotokat egy inputból:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thYtAla6gDbi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "2b89e45c-350f-4ab4-862e-d62734f61099"
      },
      "source": [
        "input_layer = model.get_layer(\"input_1\")\n",
        "lstm_2_layer = model.get_layer(\"lstm_1\")\n",
        "\n",
        "hidden_states_fun = K.function([input_layer.input],[lstm_2_layer.output])\n",
        "\n",
        "def get_embedding(x, timestep):\n",
        "    \"\"\"Return the hidden state associated with an input at the given timestep.\n",
        "    \"\"\"\n",
        "    hidden_states = hidden_states_fun([x])[0]\n",
        "    return hidden_states[0, timestep]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7b7dbe59ea64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlstm_2_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhidden_states_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlstm_2_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKUgmCrG7Itw"
      },
      "source": [
        "def cos_sim(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "while True:\n",
        "    s1 = input(\"\\nEnter the first sentence or <return> to quit: \")\n",
        "    if s1 == \"\": break\n",
        "    s2 = input(\"\\nEnter the second sentence: \")\n",
        "    try:\n",
        "        x1, l1 = str_to_input(s1)\n",
        "        x2, l2 = str_to_input(s2)\n",
        "        e1 = get_embedding(x1, l1-1)\n",
        "        e2 = get_embedding(x2, l2-1)\n",
        "        print(\"The cosine similarity between the two sentences is\", cos_sim(e1, e2))\n",
        "    except KeyError:\n",
        "        print(\"Unknown words -- please try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-3W1oxZv4DI"
      },
      "source": [
        "## Demó 3: Mini search engine\n",
        "\n",
        "A Spotify által kiadott [Annoy](https://github.com/spotify/annoy) library segítségével felhasználjuk a tréningezett modellt arra, hogy a Brown corpus minden mondatához egy LSTM belső reprezentációjából származó vektort rendeljünk, majd szomszédossági kereséssel mini keresőt hozzunk létre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWp-rofueeS0"
      },
      "source": [
        "def brown_sent_to_input(ids):\n",
        "  ids_array = np.asarray(ids)\n",
        "  length = min(max_input_length, len(ids_array))\n",
        "  result = np.zeros((1, max_input_length))\n",
        "  result[0, :length] = ids_array[:length]\n",
        "  return result, length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wda7o8ntfov1"
      },
      "source": [
        "sentlist = list(br.sentences())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaAJ7Pz5i8Cp"
      },
      "source": [
        "!pip install annoy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8SO85y_w8sX"
      },
      "source": [
        "INDEX_COVERAGE_PERCENT = 1.0 #How much of the corpus you want ot index? 1.0 means whole, 0.5 means half.\n",
        "NEAREST_NEIGHBOR_NUM = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfwnfqYff6h8"
      },
      "source": [
        "from annoy import AnnoyIndex\n",
        "from tqdm import tqdm\n",
        "\n",
        "index = AnnoyIndex(512, metric=\"angular\")\n",
        "\n",
        "for i in tqdm(range(len(int(sentlist*INDEX_COVERAGE_PERCENT)))):\n",
        "  inputs,length = brown_sent_to_input(sentlist[i])\n",
        "  vector = get_embedding(inputs, length-1)\n",
        "  index.add_item(i,vector)\n",
        "\n",
        "print(\"Building index...\")\n",
        "index.build(100)\n",
        "print(\"Index done, ready to query!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex6AGjVrh6Sb"
      },
      "source": [
        "def print_brown_index(sentences, indices):\n",
        "  for i in indices:\n",
        "    word_ids_list = sentences[i]\n",
        "    for j in word_ids_list:\n",
        "      print(br.id_to_word_map[j]+\" \", end='')\n",
        "    print()\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6y-1aVGi573",
        "scrolled": true
      },
      "source": [
        "while True:\n",
        "  query = input(\"\\nEnter the query or <return> to quit: \")\n",
        "  if query == \"\": break\n",
        "  try:\n",
        "    in_ids, length = str_to_input(query)\n",
        "    in_vector = get_embedding(in_ids, length-1)\n",
        "    nearest_sentence_indices = index.get_nns_by_vector(in_vector, NEAREST_NEIGHBOR_NUM)\n",
        "    #print(\"nearest indices:\", nearest_sentence_indices)\n",
        "    print_brown_index(sentlist, nearest_sentence_indices)\n",
        "\n",
        "  except KeyError:\n",
        "    print(\"Unknown words -- please try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}